Function: sumnum
Section: sums
C-Name: sumnum0
Prototype: V=GGEDGD0,L,p
Help: sumnum(X=a,sig,expr,{tab},{flag=0}): numerical summation of expr from
 X = ceiling(a) to +infinity. sig is either a scalar or a two-component vector
 coding the function's decrease rate at infinity. It is assumed that the
 scalar part of sig is to the right of all poles of expr. If present, tab
 must be initialized by sumnuminit. If flag is nonzero, assumes that
 conj(expr(z)) = expr(conj(z)).
Wrapper: (,,G)
Description:
  (gen,gen,gen,?gen,?small):gen:prec sumnum(${3 cookie}, ${3 wrapper}, $1, $2, $4, $5, prec)
Doc: numerical summation of \var{expr}, the variable $X$ taking integer values
 from ceiling of $a$ to $+\infty$, where \var{expr} is assumed to be a
 holomorphic function $f(X)$ for $\Re(X)\ge \sigma$.

 The parameter $\sigma\in\R$ is coded in the argument \kbd{sig} as follows: it
 is either

 \item a real number $\sigma$. Then the function $f$ is assumed to
 decrease at least as $1/X^2$ at infinity, but not exponentially;

 \item a two-component vector $[\sigma,\alpha]$, where $\sigma$ is as
 before, $\alpha < -1$. The function $f$ is assumed to decrease like
 $X^{\alpha}$. In particular, $\alpha\le-2$ is equivalent to no $\alpha$ at all.

 \item a two-component vector $[\sigma,\alpha]$, where $\sigma$ is as
 before, $\alpha > 0$. The function $f$ is assumed to decrease like
 $\exp(-\alpha X)$. In this case it is essential that $\alpha$ be exactly the
 rate of exponential decrease, and it is usually a good idea to increase
 the default value of $m$ used for the integration step. In practice, if
 the function is exponentially decreasing \kbd{sumnum} is slower and less
 accurate than \kbd{sumpos} or \kbd{suminf}, so should not be used.

 The function uses the \tet{intnum} routines and integration on the line
 $\Re(s) = \sigma$. The optional argument \var{tab} is as in intnum, except it
 must be initialized with \kbd{sumnuminit} instead of \kbd{intnuminit}.

 When \var{tab} is not precomputed, \kbd{sumnum} can be slower than
 \kbd{sumpos}, when the latter is applicable. It is in general faster for
 slowly decreasing functions.


 Finally, if $\fl$ is nonzero, we assume that the function $f$ to be summed is
 of real type, i.e. satisfies $\overline{f(z)}=f(\overline{z})$, which
 speeds up the computation.

 \bprog
 ? \p 308
 ? a = sumpos(n=1, 1/(n^3+n+1));
 time = 188 ms.
 ? tab = sumnuminit(2);
 time = 184 ms. \\@com done once and for all
 ? b = sumnum(n=1, 2, 1/(n^3+n+1), tab);
 time = 52 ms. \\@com 3 times faster than \kbd{sumpos}
 ? a - b
 %4 = 1.1... E-307 + 0.E-331*I \\@com perfect.
 ? sumnum(n=1, 2, 1/(n^3+n+1), tab, 1) - a \\@com function of real type
 time = 32 ms.
 %2 = -1.1... E-307 \\@com twice as fast, no imaginary part.
 ? c = sumnum(n=1, 2, 1/(n^2+1), tab, 1);
 time = 24 ms. \\@com fast
 ? d = sumpos(n=1, 1 / (n^2+1));
 time = 312 ms. \\@com slow.
 ? e = sumpos(n=1, 1 / (n^2+1), 1);
 time = 248 ms. \\@com slow.
 ? d - c
 %5 = -6.6... E-308 \\@com perfect.
 @eprog

 For slowly decreasing function, we must indicate singularities:
 \bprog
 ? \p 308
 ? a = sumnum(n=1, 2, n^(-4/3));
 time = 1,076 ms. \\@com slow because of the computation of $n^{-4/3}$.
 ? a - zeta(4/3)
 time = 56 ms.
 %1 = -1.6... E-110 \\@com lost 200 decimals because of singularity at $\infty$
 ? b = sumnum(n=1, [2,-4/3], n^(-4/3), /*omitted*/, 1); \\@com of real type
 time = 1,396 ms.
 ? b - zeta(4/3)
 %3 = -7.0.. E-235 \\@com better but still 70 decimal missing
 ? b = sumnum(n=1, [2,-4/3], n^(-4/3), 1,1); \\@com number of sampling point x 2
 time = 2,729 ms.
 ? b - zeta(4/3)
 %5 = -3.5.. E-305 \\@com good
 @eprog

 Since \emph{complex} values of the function are used, beware of
 determination problems. For instance:
 \bprog
 ? \p 308
 ? tab = sumnuminit([2,-3/2]);
 time = 192 ms.
 ? sumnum(n=1,[2,-3/2], 1/(n*sqrt(n)), tab,1) - zeta(3/2)
 time = 92 ms.
 %1 = -1.4... E-289 \\@com fast and almost correct
 ? sumnum(n=1,[2,-3/2], 1/sqrt(n^3), tab,1) - zeta(3/2)
 time = 730 ms.
 %2 = -1.55... \\@com nonsense. However
 ? sumnum(n=1,[2,-3/2], 1/n^(3/2), tab,1) - zeta(3/2)
 time = 1,060 ms.
 %3 = -1.4... E-289 \\@com as $1/(n*\sqrt{n})$ above but much slower
 @eprog

 For exponentially decreasing functions, \kbd{sumnum} is given for
 completeness, but one of \tet{suminf} or \tet{sumpos} should always be
 preferred. If you experiment with such functions and \kbd{sumnum} anyway,
 indicate the exact rate of decrease and set $m=1$ or $2$:
 \bprog
 ? suminf(n=1, 2^(-n)) - 1
 time = 4 ms.
 %1 = -1.11... E-308 \\@com fast and perfect
 ? sumpos(n=1, 2^(-n)) - 1
 time = 4 ms.
 %2 = -2.78... E-308 \\@com also fast and perfect
 ? sumnum(n=1,2, 2^(-n)) - 1 \\@com nonsense
  ***   at top-level: sumnum(n=1,2,2^(-n))-1
  ***                               ^--------
  *** _^_: precision too low in mpcosm1.
 ? sumnum(n=1, [2,log(2)], 2^(-n), /*omitted*/, 1) - 1 \\@com of real type
 time = 537 ms.
 %4 = 1.11... E-177 \\@com slow and lost $130$ decimals
 ? sumnum(n=1,[2,log(2)], 2^(-n), m=1, 1) - 1
 time = 1,068 ms.
 %6 = 7.8... E-306 \\@com now perfect, but slow.
 @eprog

 \synt{sumnum}{void *E, GEN (*eval)(void*,GEN), GEN a,GEN sig,GEN tab,long flag, long prec}.
